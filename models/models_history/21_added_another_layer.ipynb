{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.io import imshow\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18212658696122530059\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11285289370\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16595455417703032083\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#making sure GPU is in use \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "in_height = 64\n",
    "in_width = 64\n",
    "out_height = 256\n",
    "out_width = 256\n",
    "color_dim = 3\n",
    "path_in =  '/home/Matthew/image-super-resolution/data/imagenet/40k/res64noisy/'\n",
    "path_out = '/home/Matthew/image-super-resolution/data/imagenet/40k/res256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: add random shuffle . seed = 0\n",
    "\n",
    "def load_images(path_in, path_out, test_size, valid_size):\n",
    "    images = next(os.walk(path_in))[2] #oswalk is a generator \n",
    "    num_images = len(images)\n",
    "  \n",
    "    testSize  = int(test_size * num_images)\n",
    "    validSize = int(valid_size * num_images)\n",
    "    trainSize = int(math.ceil((1-(test_size+valid_size)) * num_images))\n",
    "    \n",
    "    X_train = np.zeros((trainSize,in_height,in_width,color_dim), dtype=np.float32)\n",
    "    X_test = np.zeros((testSize,in_height,in_width,color_dim), dtype=np.float32)\n",
    "    X_valid = np.zeros((validSize,in_height,in_width,color_dim), dtype=np.float32)\n",
    "\n",
    "    y_train = np.zeros((trainSize,out_height,out_width,color_dim), dtype=np.float32)\n",
    "    y_test  = np.zeros((testSize,out_height,out_width,color_dim), dtype=np.float32)\n",
    "    y_valid  = np.zeros((validSize,out_height,out_width,color_dim), dtype=np.float32)\n",
    "    \n",
    "    trainIdx = 0\n",
    "    testIdx = 0\n",
    "    validIdx = 0\n",
    "    for idx, image in enumerate(images):\n",
    "        image_in_path = os.path.join(path_in,image)\n",
    "        image_out_path = os.path.join(path_out,image)\n",
    "\n",
    "        image_in_raw =  load_img(image_in_path, grayscale=False)\n",
    "        image_out_raw = load_img(image_out_path, grayscale=False)\n",
    "\n",
    "        #converts image to keras preprocessing image, then divide by 255 to un-invert the images\n",
    "        image_in = (img_to_array(image_in_raw)).squeeze() / 255 \n",
    "        image_out = (img_to_array(image_out_raw)).squeeze() / 255          \n",
    " \n",
    "        try:\n",
    "            if (idx % 1000 == 0):\n",
    "                print(\"Stage \" + str(idx))\n",
    "        \n",
    "            if (idx < testSize):\n",
    "                #print(\"testIdx \" + str(testIdx))\n",
    "                X_test[testIdx] = image_in\n",
    "                y_test[testIdx] = image_out\n",
    "                testIdx+=1\n",
    "                \n",
    "            elif (idx < testSize + validSize):\n",
    "                #print(\"validIdx \" + str(validIdx))\n",
    "                X_valid[validIdx] = image_in\n",
    "                y_valid[validIdx] = image_out\n",
    "                validIdx+=1\n",
    "                \n",
    "            else:\n",
    "                #print(\"trainIdx \" + str(trainIdx))\n",
    "                X_train[trainIdx] = image_in\n",
    "                y_train[trainIdx] = image_out\n",
    "                trainIdx+=1\n",
    "                \n",
    "        except Exception as e: \n",
    "            print(\"\\nERROR!!!!\")\n",
    "            print(image_in.shape)\n",
    "            print(trainIdx)\n",
    "            \n",
    "            print(image_out.shape)\n",
    "            print(testIdx)\n",
    "            \n",
    "            print(\"id \"+str(idx))\n",
    "            print(image_in_path)\n",
    "            print(e)\n",
    "           \n",
    "            print(\"\\n\")\n",
    "            \n",
    "    return X_train,X_valid,X_test,y_train,y_valid,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0\n",
      "Stage 1000\n",
      "Stage 2000\n",
      "Stage 3000\n",
      "Stage 4000\n",
      "Stage 5000\n",
      "Stage 6000\n",
      "Stage 7000\n",
      "Stage 8000\n",
      "Stage 9000\n",
      "Stage 10000\n",
      "Stage 11000\n",
      "Stage 12000\n",
      "Stage 13000\n",
      "Stage 14000\n",
      "Stage 15000\n",
      "Stage 16000\n",
      "Stage 17000\n",
      "Stage 18000\n",
      "Stage 19000\n",
      "Stage 20000\n",
      "Stage 21000\n",
      "Stage 22000\n",
      "Stage 23000\n",
      "Stage 24000\n",
      "Stage 25000\n",
      "Stage 26000\n",
      "Stage 27000\n",
      "Stage 28000\n",
      "Stage 29000\n",
      "Stage 30000\n",
      "Stage 31000\n",
      "Stage 32000\n",
      "Stage 33000\n",
      "Stage 34000\n",
      "Stage 35000\n",
      "Stage 36000\n",
      "Stage 37000\n",
      "Stage 38000\n",
      "Stage 39000\n",
      "Stage 40000\n"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,X_test,y_train,y_valid,y_test = load_images(path_in, path_out, .2, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Activation\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers import  BatchNormalization, Activation, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padding output size: see http://cs231n.github.io/convolutional-networks/\n",
    "# output = (Wâˆ’F+2P)/S+1   w- input size, f-kernel size, p-padding size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#inputShape = (in_height, in_width, color_dim)\n",
    "#model.add(UpSampling2D((4,4),input_shape=inputShape))\n",
    "#model.add(Conv2D(32, (9, 9), padding='same', kernel_initializer=\"he_normal\")) # 256 X 256 X 32\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(Conv2D(16, (7, 7), padding='same', kernel_initializer=\"he_normal\")) # 256 X 256 X 16\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(Conv2D(16, (7, 7), padding='same', kernel_initializer=\"he_normal\")) # 256 X 256 X 16\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(Conv2D(color_dim, (5, 5), padding='same', kernel_initializer=\"he_normal\")) # 256 X 256 X 3\n",
    "#model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.depends-on-the-definition.com/unet-keras-segmenting-images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    # first layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # second layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
    "    # contracting path\n",
    "    #c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    \n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(4, 4), padding='same')(input_img)\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), padding='same') (c6)\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), padding='same') (c7)\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), padding='same') (c8)\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(color_dim, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_img = Input((in_height, in_width, color_dim), name='img')\n",
    "model = create_network(input_img, n_filters=32, dropout=0.05, batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer = RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 256, 256, 256)     7168      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 256, 256, 256)     590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 256, 256, 256)     1024      \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 256, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 256, 256, 256)     590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 256, 256, 256)     1024      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 256, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 256, 256, 128)     295040    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 256, 256, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 256, 256, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DT (None, 256, 256, 64)      73792     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 256, 256, 32)      18464     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 256, 256, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 256, 256, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 256, 256, 3)       99        \n",
      "=================================================================\n",
      "Total params: 1,966,083\n",
      "Trainable params: 1,964,163\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model_21.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24423 samples, validate on 8141 samples\n",
      "Epoch 1/30\n",
      "24423/24423 [==============================] - 10067s 412ms/step - loss: 0.0128 - acc: 0.6017 - val_loss: 0.0096 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00960, saving model to model_21.h5\n",
      "Epoch 2/30\n",
      "24423/24423 [==============================] - 10065s 412ms/step - loss: 0.0093 - acc: 0.6667 - val_loss: 0.0083 - val_acc: 0.6589\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00960 to 0.00831, saving model to model_21.h5\n",
      "Epoch 3/30\n",
      "24423/24423 [==============================] - 10063s 412ms/step - loss: 0.0085 - acc: 0.6831 - val_loss: 0.0075 - val_acc: 0.6972\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00831 to 0.00751, saving model to model_21.h5\n",
      "Epoch 4/30\n",
      "24423/24423 [==============================] - 10060s 412ms/step - loss: 0.0080 - acc: 0.6937 - val_loss: 0.0071 - val_acc: 0.7134\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00751 to 0.00711, saving model to model_21.h5\n",
      "Epoch 5/30\n",
      "24423/24423 [==============================] - 10036s 411ms/step - loss: 0.0077 - acc: 0.7001 - val_loss: 0.0072 - val_acc: 0.7126\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00711\n",
      "Epoch 6/30\n",
      "24423/24423 [==============================] - 10069s 412ms/step - loss: 0.0075 - acc: 0.7064 - val_loss: 0.0071 - val_acc: 0.7227\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00711\n",
      "Epoch 7/30\n",
      "24423/24423 [==============================] - 10041s 411ms/step - loss: 0.0074 - acc: 0.7105 - val_loss: 0.0071 - val_acc: 0.7291\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00711 to 0.00708, saving model to model_21.h5\n",
      "Epoch 8/30\n",
      "24423/24423 [==============================] - 10041s 411ms/step - loss: 0.0070 - acc: 0.7213 - val_loss: 0.0068 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00708 to 0.00679, saving model to model_21.h5\n",
      "Epoch 9/30\n",
      "24423/24423 [==============================] - 10032s 411ms/step - loss: 0.0069 - acc: 0.7218 - val_loss: 0.0069 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00679\n",
      "Epoch 10/30\n",
      "24423/24423 [==============================] - 10024s 410ms/step - loss: 0.0069 - acc: 0.7234 - val_loss: 0.0071 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00679\n",
      "Epoch 11/30\n",
      "24423/24423 [==============================] - 10042s 411ms/step - loss: 0.0069 - acc: 0.7232 - val_loss: 0.0069 - val_acc: 0.7345\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00679\n",
      "Epoch 12/30\n",
      "24423/24423 [==============================] - 10093s 413ms/step - loss: 0.0068 - acc: 0.7254 - val_loss: 0.0068 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00679\n",
      "Epoch 13/30\n",
      "24423/24423 [==============================] - 10096s 413ms/step - loss: 0.0069 - acc: 0.7248 - val_loss: 0.0068 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00679 to 0.00679, saving model to model_21.h5\n",
      "Epoch 14/30\n",
      "24423/24423 [==============================] - 10059s 412ms/step - loss: 0.0069 - acc: 0.7256 - val_loss: 0.0068 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00679\n",
      "Epoch 15/30\n",
      "24423/24423 [==============================] - 10078s 413ms/step - loss: 0.0069 - acc: 0.7252 - val_loss: 0.0068 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00679\n",
      "Epoch 16/30\n",
      "24423/24423 [==============================] - 10096s 413ms/step - loss: 0.0068 - acc: 0.7254 - val_loss: 0.0067 - val_acc: 0.7457\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00679 to 0.00675, saving model to model_21.h5\n",
      "Epoch 17/30\n",
      "24423/24423 [==============================] - 10080s 413ms/step - loss: 0.0068 - acc: 0.7257 - val_loss: 0.0068 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00675\n",
      "Epoch 18/30\n",
      "24423/24423 [==============================] - 10153s 416ms/step - loss: 0.0068 - acc: 0.7256 - val_loss: 0.0068 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00675\n",
      "Epoch 19/30\n",
      "24423/24423 [==============================] - 10143s 415ms/step - loss: 0.0068 - acc: 0.7255 - val_loss: 0.0068 - val_acc: 0.7448\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00675\n",
      "Epoch 20/30\n",
      "24423/24423 [==============================] - 10140s 415ms/step - loss: 0.0068 - acc: 0.7254 - val_loss: 0.0068 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00675\n",
      "Epoch 21/30\n",
      "24423/24423 [==============================] - 10134s 415ms/step - loss: 0.0068 - acc: 0.7257 - val_loss: 0.0067 - val_acc: 0.7461\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00675 to 0.00674, saving model to model_21.h5\n",
      "Epoch 22/30\n",
      "24423/24423 [==============================] - 10121s 414ms/step - loss: 0.0068 - acc: 0.7257 - val_loss: 0.0068 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00674\n",
      "Epoch 23/30\n",
      "24423/24423 [==============================] - 10133s 415ms/step - loss: 0.0068 - acc: 0.7258 - val_loss: 0.0068 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00674\n",
      "Epoch 24/30\n",
      "24423/24423 [==============================] - 10142s 415ms/step - loss: 0.0068 - acc: 0.7259 - val_loss: 0.0068 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00674\n",
      "Epoch 25/30\n",
      "24423/24423 [==============================] - 10130s 415ms/step - loss: 0.0068 - acc: 0.7262 - val_loss: 0.0068 - val_acc: 0.7486\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00674\n",
      "Epoch 26/30\n",
      "24423/24423 [==============================] - 10106s 414ms/step - loss: 0.0068 - acc: 0.7260 - val_loss: 0.0068 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00674\n",
      "Epoch 27/30\n",
      "24423/24423 [==============================] - 10128s 415ms/step - loss: 0.0068 - acc: 0.7268 - val_loss: 0.0068 - val_acc: 0.7460\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00674\n",
      "Epoch 28/30\n",
      "19112/24423 [======================>.......] - ETA: 32:42 - loss: 0.0068 - acc: 0.7270"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 30\n",
    "trainHistory = model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_valid, y_valid),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = trainHistory.history['loss']\n",
    "val_loss = trainHistory.history['val_loss']\n",
    "epochsRange = range(len(trainHistory.history['loss']))\n",
    "plt.figure()\n",
    "plt.plot(epochsRange, loss, label='Training loss',color='g')\n",
    "plt.plot(epochsRange, val_loss, label='Validation loss',color='b')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(trainHistory.history['acc'])\n",
    "plt.plot(trainHistory.history['val_acc'])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainHistory.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CalcL1(obj1,obj2):\n",
    "    #obj1 = np.reshape(obj1,(obj1.shape[0]*obj1.shape[1]*obj1.shape[2],1))\n",
    "    #obj2 = np.reshape(obj2,(obj2.shape[0]*obj2.shape[1]*obj2.shape[2],1))\n",
    "    #return scipy.spatial.distance.cdist(obj1, obj2, metric='cityblock')\n",
    "    return np.absolute(obj1-obj2).sum()\n",
    "\n",
    "def CalcL2(obj1,obj2):\n",
    "    return LA.norm(obj1-obj2)\n",
    "\n",
    "\n",
    "for i in range(40):\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, 1) # 1 row, 2 columns, index in subplot\n",
    "    plt.imshow(X_test[i]) # show image to previous defined subplot\n",
    "    ax.title.set_text('Input: 64X64 noisy')\n",
    "    ax.set_ylabel('ylabel')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(pred[i])\n",
    "    ax.title.set_text('Output: 256X256 rebuilt')\n",
    "    ax.set_ylabel('ylabel')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 3) # 1 row, 2 columns, index in subplot\n",
    "    plt.imshow(y_test[i]) # show image to previous defined subplot\n",
    "    ax.title.set_text('Orig: 256X256')\n",
    "    l1Dist = CalcL1(pred[i],y_test[i])\n",
    "    l2Dist = CalcL2(pred[i],y_test[i])\n",
    "    ax.set_xlabel('L1: ' + str(l1Dist) + '  L2: ' + str(l2Dist))\n",
    "    #ax.set_xlabel(' L2: ' + str(l2Dist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
